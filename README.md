# Transformer Embedding Dynamics

This project investigated the internal dynamics of **transformer architectures**, with a focus on the transformation of **embedding spaces** across layers. The study combined a **physical and phenomenological perspective** to analyze the roles of both the **attention mechanism** and the **feed-forward networks (FFNNs)** in shaping high-dimensional representations.


## Methodology
- Analysis of embedding transformations across layers of a transformer model (GPT-like).  
- Study of **attention patterns** and their influence on representation geometry.  
- Examination of the **FFNN component** and its nonlinear contribution to embedding evolution.  
- Application of dimensionality reduction and visualization techniques to interpret structural properties.  


## Results
- Characterized the evolution of embedding spaces under attention and FFNN transformations.  
- Provided insights into the structural and geometric features underlying transformer representations.  
- Highlighted interpretability challenges and potential directions for representation analysis.  


## Authors
This work was carried out in collaboration between:  
- **Alessandro Miotto**  
- **Lorenzo Rizzi**  
- **Riccardo Corte**  
